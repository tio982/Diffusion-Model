# -*- coding: utf-8 -*-
"""train

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16kUEgOMglWGzmeqdw5DP1uGdCfkKP5-B
"""

from tqdm import tqdm
import argparse
import torch
import time
import wandb
from data import load_data
from model import create_model
from utils import save_images_as_artifact, save_images_locally, calculate_fid, read_and_preprocess_images

def main(checkpoint_path=None):
    # Set up Weights & Biases
    wandb.init(project='AML_Group_Project', entity='AMLgroup17', name='Diffusion Model: Final run')
    original_images_artifact = wandb.run.use_artifact('original_images:latest')

    # Define the local folder path to save the original images
    original_images_folder_path = '/content/original_images'
    original_images_artifact.download(root=original_images_folder_path) 

    # Initialize current step
    current_step = 0
    
    # Define hyperparameters
    save_model_every = 10
    batch_size = 32
    num_epochs = 40
    learning_rate = 0.0001
    
    # Load data
    train_loader = load_data()

    # Create model and training components
    model, loss_fn, optimizer, scheduler = create_model(learning_rate)

    # Set device to 'cuda' if a GPU is available, otherwise use 'cpu'
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Move model and loss function to the correct device
    model.to(device)
    loss_fn.to(device)

    # Load model from checkpoint if specified
    if checkpoint_path:
        checkpoint = torch.load(checkpoint_path)
        model.load_state_dict(checkpoint)
        current_step = checkpoint.get('current_step', 0)
        print(f"Loaded model from checkpoint: {checkpoint_path}")
    else:
        print("Starting training from scratch")

    trainingstart_time = time.time()
    # Training Loop
    for epoch in range(num_epochs):
        # Start timer for the epoch
        start_time = time.time()
        
        print(f"\nEpoch [{epoch + 1}/{num_epochs}]")

        model.train()

        # Initialize loss for the epoch
        epoch_loss = 0
        
        # Use tqdm to wrap the loop and display a progress bar
        with tqdm(total=len(train_loader), desc=f"Epoch {epoch + 1}/{num_epochs}") as pbar:
            for batch_idx, (data, _) in enumerate(train_loader):
                # Move data to device
                data = data.to(device)
                optimizer.zero_grad()

                # Forward pass
                timestep = 100  # Define timestep here
                output = model(data, timestep)

                # Calculate loss
                loss = loss_fn(output.sample, data)
                epoch_loss += loss.item()

                # Backward pass
                loss.backward()
                optimizer.step()

                # Update scheduler
                scheduler.step(output.sample, timestep, data)

                # Log metrics using W&B at every step
                wandb.log({"loss": loss.item()}, step=current_step)

                # Visualize generated images at frequent intervals
                if batch_idx % 100 == 0:
                    # Generate sample images
                    num_samples = 5
                    generated_images = output.sample

                    # Log generated images to W&B
                    wandb.log({"generated_images": [wandb.Image(img) for img in generated_images]}, step=current_step)

                # Increment current step
                current_step += 1

                # Update the progress bar
                pbar.update(1)
                pbar.set_postfix(loss=loss.item())

        # Calculate average loss for the epoch
        avg_epoch_loss = epoch_loss / len(train_loader)
        print(f" - Epoch loss: {avg_epoch_loss:.4f}")

        # Save images to local storage
        save_images_locally(generated_images, f'/content/generated_images_epoch_{epoch + 1}')

        # Save generated images as a W&B artifact
        save_images_as_artifact(generated_images, f'generated_images_epoch_{epoch + 1}')

        # Every 5 epochs: Calculate FID and IS scores
        if (epoch + 1) % 5 == 0:
            original_images = read_and_preprocess_images(original_images_folder_path)
            generated_images = read_and_preprocess_images(f'/content/generated_images_epoch_{epoch + 1}')
            
            fid_score, inception_score = calculate_fid(original_images, generated_images)
            print(f"\nFID Score: {fid_score:.2f}")
            print(f"Inception Score: {inception_score:.2f}")

            # Log the scores using W&B
            wandb.log({"FID": fid_score, "IS": inception_score}, step=current_step)
        
        # Every 10 epochs: Save the checkpoint
        if (epoch + 1) % save_model_every == 0:
            # Save model checkpoint
            model_checkpoint_path = f"model_checkpoint.pt"
            torch.save(model.state_dict(), model_checkpoint_path)

            # Log model checkpoint as W&B artifact
            artifact = wandb.Artifact(f"model_epoch_{epoch + 1}", type="model")
            artifact.add_file(model_checkpoint_path)
            wandb.log_artifact(artifact)

        # Calculate epoch duration and print time taken
        epoch_duration = time.time() - start_time
        print(f" - Epoch duration: {epoch_duration:.2f} seconds")

     # Measure the ending time
    trainingend_time = time.time()

    # Calculate and print the total training time
    total_training_time = trainingend_time - trainingstart_time

    print(f"\nTotal training time: {total_training_time:.2f} seconds")

    # Log the total training time to W&B
    wandb.log({"total_training_time": total_training_time})

    wandb.finish()

if __name__ == "__main__":
    # Add command-line argument parsing
    parser = argparse.ArgumentParser(description="Train a model with optional checkpoint loading.")
    parser.add_argument("--checkpoint_path", type=str, default=None, help="Path to the model checkpoint file. Set to None to train from scratch.")
    args = parser.parse_args()

    # Call the main function with the parsed checkpoint path argument
    main(args.checkpoint_path)